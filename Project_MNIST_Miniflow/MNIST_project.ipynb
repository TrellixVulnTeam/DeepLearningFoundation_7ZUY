{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Project with miniflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### miniflow is a mini library from Udacity Deep Learning Foundation Nanodegree. In this course, this library only have few function for linear, sgd_update, topological_sort. I try to add more function to let this library can be flexible and fit MNIST project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from miniflow import *\n",
    "import gzip\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key_file = {\n",
    "    'train_img':'train-images-idx3-ubyte.gz',\n",
    "    'train_label':'train-labels-idx1-ubyte.gz',\n",
    "    'test_img':'t10k-images-idx3-ubyte.gz',\n",
    "    'test_label':'t10k-labels-idx1-ubyte.gz'\n",
    "}\n",
    "\n",
    "\n",
    "def load_img(file_name):\n",
    "    file_path = \"./\" + file_name\n",
    "\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "\n",
    "    data = data.reshape(-1, 784)\n",
    "    data = data.reshape(-1, 1, 28, 28)\n",
    "    print(\"image done\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def load_label(file_name):\n",
    "    file_path = \"./\" + file_name\n",
    "\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "            labels = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    print(\"load label done\")\n",
    "\n",
    "    return labels\n",
    "\n",
    "def one_hot_encoding(y):\n",
    "    n_values = np.max(y) + 1\n",
    "    y = np.eye(n_values, dtype=int)[y]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image done\n",
      "load label done\n",
      "image done\n",
      "load label done\n"
     ]
    }
   ],
   "source": [
    "dataset = {}\n",
    "dataset['train_img'] =  load_img(key_file['train_img'])\n",
    "dataset['train_label'] = load_label(key_file['train_label'])\n",
    "dataset['test_img'] = load_img(key_file['test_img'])\n",
    "dataset['test_label'] = load_label(key_file['test_label'])\n",
    "\n",
    "X_ = normalized(dataset['train_img'])\n",
    "X_test = normalized(dataset['test_img'])\n",
    "\n",
    "y_ = one_hot_encoding(dataset['train_label'])\n",
    "#y_test = one_hot_encoding(dataset['test_label'])\n",
    "y_test = dataset['test_label']\n",
    "\n",
    "# parameters\n",
    "fitter_numbers = 16\n",
    "kernel_size = (3,3)\n",
    "\n",
    "# init layers\n",
    "W_layer1 = np.sqrt(2.0/(1*3*3)) * np.random.randn(fitter_numbers, 1, kernel_size[0], kernel_size[1])\n",
    "b_layer1 = np.zeros(fitter_numbers)\n",
    "\n",
    "W_layer2 = np.sqrt(2.0/(16*3*3)) * np.random.randn(fitter_numbers, 16, kernel_size[0], kernel_size[1])\n",
    "b_layer2 = np.zeros(fitter_numbers)\n",
    "\n",
    "fitter_numbers = 32\n",
    "\n",
    "W_layer3 = np.sqrt(2.0/(16*3*3)) * np.random.randn(fitter_numbers, 16, kernel_size[0], kernel_size[1])\n",
    "b_layer3 = np.zeros(fitter_numbers)\n",
    "\n",
    "W_layer4 = np.sqrt(2.0/(32*3*3)) * np.random.randn(fitter_numbers, 32, kernel_size[0], kernel_size[1])\n",
    "b_layer4 = np.zeros(fitter_numbers)\n",
    "\n",
    "fitter_numbers = 64\n",
    "\n",
    "W_layer5 = np.sqrt(2.0/(32*3*3)) * np.random.randn(fitter_numbers, 32, kernel_size[0], kernel_size[1])\n",
    "b_layer5 = np.zeros(fitter_numbers)\n",
    "\n",
    "W_layer6 = np.sqrt(2.0/(64*3*3)) * np.random.randn(fitter_numbers, 64, kernel_size[0], kernel_size[1])\n",
    "b_layer6 = np.zeros(fitter_numbers)\n",
    "\n",
    "W_layer7 = np.sqrt(2.0/(64*4*4)) * np.random.randn(64*4*4, 50)\n",
    "b_layer7 = np.zeros(50)\n",
    "\n",
    "W_layer8 = np.sqrt(2.0 / 50) * np.random.randn(50, 10)\n",
    "b_layer8 = np.zeros(10)\n",
    "\n",
    "# network\n",
    "X, y = Input(), Input()\n",
    "W1, b1 = Input(), Input()\n",
    "W2, b2 = Input(), Input()\n",
    "W3, b3 = Input(), Input()\n",
    "W4, b4 = Input(), Input()\n",
    "W5, b5 = Input(), Input()\n",
    "W6, b6 = Input(), Input()\n",
    "W7, b7 = Input(), Input()\n",
    "W8, b8 = Input(), Input()\n",
    "\n",
    "conv_layer1 = Conv(X, W1, b1, (1,1), 1)\n",
    "\n",
    "activation_1 = Relu(conv_layer1)\n",
    "\n",
    "conv_layer2 = Conv(activation_1, W2, b2, (1,1), 1)\n",
    "\n",
    "activation_2 = Relu(conv_layer2)\n",
    "\n",
    "pooling1 = Pooling(activation_2, (2,2), (2,2), 0)\n",
    "\n",
    "conv_layer3 = Conv(pooling1, W3, b3, (1,1), 1)\n",
    "\n",
    "activation_3 = Relu(conv_layer3)\n",
    "\n",
    "conv_layer4 = Conv(activation_3, W4, b4, (1,1), 2)\n",
    "\n",
    "activation_4 = Relu(conv_layer4)\n",
    "\n",
    "pooling2 = Pooling(activation_4, (2,2), (2,2), 0)\n",
    "\n",
    "conv_layer5 = Conv(pooling2, W5, b5, (1,1), 1)\n",
    "\n",
    "activation_5 = Relu(conv_layer5)\n",
    "\n",
    "conv_layer6 = Conv(activation_5, W6, b6, (1,1), 1)\n",
    "\n",
    "activation_6 = Relu(conv_layer6)\n",
    "\n",
    "pooling3 = Pooling(activation_6, (2,2), (2,2), 0)\n",
    "\n",
    "linear1 = Linear(pooling3, W7, b7)\n",
    "\n",
    "activation_7 = Relu(linear1)\n",
    "\n",
    "dropout1 = Dropout(activation_7, 0.5)\n",
    "\n",
    "linear2 = Linear(dropout1, W8, b8)\n",
    "\n",
    "dropout2 = Dropout(linear2, 0.5)\n",
    "\n",
    "output = Softmax(dropout2, y)\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W1:W_layer1,\n",
    "    b1:b_layer1,\n",
    "    W2:W_layer2,\n",
    "    b2:b_layer2,\n",
    "    W3:W_layer3,\n",
    "    b3:b_layer3,\n",
    "    W4:W_layer4,\n",
    "    b4:b_layer4,\n",
    "    W5:W_layer5,\n",
    "    b5:b_layer5,\n",
    "    W6:W_layer6,\n",
    "    b6:b_layer6,\n",
    "    W7:W_layer7,\n",
    "    b7:b_layer7,\n",
    "    W8:W_layer8,\n",
    "    b8:b_layer8}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "\n",
    "trainables = [W1, b1, W2, b2, W3, b3, W4, b4, W5, b5, W6, b6, W7, b7, W8, b8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 / 600 : 1.79307567989\n",
      "200 / 600 : 1.70618253166\n",
      "300 / 600 : 1.73685830379\n",
      "400 / 600 : 1.49831058727\n",
      "500 / 600 : 1.55286748452\n",
      "600 / 600 : 1.63061037233\n",
      "Epoch: 1, Loss: 1.652\n",
      "Epoch: 1, test acc: 0.940\n",
      "100 / 600 : 1.34238390056\n",
      "200 / 600 : 1.21045248909\n",
      "300 / 600 : 1.61688997457\n",
      "400 / 600 : 1.41278497827\n",
      "500 / 600 : 1.363536069\n",
      "600 / 600 : 1.27359409953\n",
      "Epoch: 2, Loss: 1.286\n",
      "Epoch: 2, test acc: 0.950\n",
      "100 / 600 : 1.16867455331\n",
      "200 / 600 : 1.26152609982\n",
      "300 / 600 : 1.53818759614\n",
      "400 / 600 : 1.12506756629\n",
      "500 / 600 : 1.09868925538\n",
      "600 / 600 : 1.26185131321\n",
      "Epoch: 3, Loss: 1.169\n",
      "Epoch: 3, test acc: 0.950\n",
      "100 / 600 : 0.770464271826\n",
      "200 / 600 : 0.961347489709\n",
      "300 / 600 : 1.21743037097\n",
      "400 / 600 : 1.22170012179\n",
      "500 / 600 : 1.2172622532\n",
      "600 / 600 : 1.10395191703\n",
      "Epoch: 4, Loss: 1.099\n",
      "Epoch: 4, test acc: 0.940\n",
      "100 / 600 : 1.14063808559\n",
      "200 / 600 : 0.858504260799\n",
      "300 / 600 : 1.04406570444\n",
      "400 / 600 : 1.3955724041\n",
      "500 / 600 : 1.06546558387\n",
      "600 / 600 : 1.02590403381\n",
      "Epoch: 5, Loss: 1.066\n",
      "Epoch: 5, test acc: 0.980\n",
      "100 / 600 : 0.993942905343\n",
      "200 / 600 : 0.934651013336\n",
      "300 / 600 : 1.28641834533\n",
      "400 / 600 : 1.24422314466\n",
      "500 / 600 : 0.955289223001\n",
      "600 / 600 : 0.937503527738\n",
      "Epoch: 6, Loss: 1.034\n",
      "Epoch: 6, test acc: 1.000\n",
      "100 / 600 : 0.878464801507\n",
      "200 / 600 : 0.894196642034\n",
      "300 / 600 : 1.17874540381\n",
      "400 / 600 : 1.02234396701\n",
      "500 / 600 : 0.923765148455\n",
      "600 / 600 : 1.08087488653\n",
      "Epoch: 7, Loss: 1.011\n",
      "Epoch: 7, test acc: 0.990\n",
      "100 / 600 : 1.05064096956\n",
      "200 / 600 : 0.901437160509\n",
      "300 / 600 : 1.09215373757\n",
      "400 / 600 : 0.903788975605\n",
      "500 / 600 : 1.0352542168\n",
      "600 / 600 : 1.18806125762\n",
      "Epoch: 8, Loss: 1.002\n",
      "Epoch: 8, test acc: 1.000\n",
      "100 / 600 : 0.916417876015\n",
      "200 / 600 : 0.989349111272\n",
      "300 / 600 : 1.00696007672\n",
      "400 / 600 : 0.954665464857\n",
      "500 / 600 : 1.05291672042\n",
      "600 / 600 : 1.09007783214\n",
      "Epoch: 9, Loss: 0.993\n",
      "Epoch: 9, test acc: 1.000\n",
      "100 / 600 : 0.880805820919\n",
      "200 / 600 : 0.978858496678\n",
      "300 / 600 : 1.04523951823\n",
      "400 / 600 : 1.01186072363\n",
      "500 / 600 : 1.00515092015\n",
      "600 / 600 : 1.07592181565\n",
      "Epoch: 10, Loss: 0.971\n",
      "Epoch: 10, test acc: 0.980\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "learning_rate=1e-2\n",
    "train_size = X_.shape[0]\n",
    "test_size = X_test.shape[0]\n",
    "batch_size = 100\n",
    "\n",
    "steps_per_epoch = int(train_size/batch_size)\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    index = 0\n",
    "    while (index + 1)*100 <= train_size:\n",
    "\n",
    "        #batch_mask = np.random.choice(train_size, batch_size)\n",
    "        X_batch = X_[index*100: (index+1)*100]\n",
    "        y_batch = y_[index*100: (index+1)*100]\n",
    "\n",
    "        X.value = X_batch\n",
    "        y.value = y_batch\n",
    "\n",
    "        forward_and_backward(graph)\n",
    "        sgd_update(trainables)\n",
    "        if (index + 1)%100 == 0:\n",
    "            print index+1,'/',steps_per_epoch,':',graph[-1].loss\n",
    "        loss += graph[-1].loss\n",
    "        index += 1\n",
    "\n",
    "    print(\"Epoch: {}, Loss: {:.3f}\".format(i+1, loss/float(steps_per_epoch)))\n",
    "    loss_list.append(loss/steps_per_epoch)\n",
    "\n",
    "    batch_mask = np.random.choice(test_size, 100)\n",
    "    X_batch_test = X_test[batch_mask]\n",
    "    y_batch_test = y_test[batch_mask]\n",
    "    X.value = X_batch_test\n",
    "    res = predict(graph)\n",
    "    curr_num = np.sum(y_batch_test == np.argmax(res, axis=1))\n",
    "    print \"Epoch: {}, test acc: {:.3f}\".format(i+1, curr_num/float(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc for all test data : 0.980 \n"
     ]
    }
   ],
   "source": [
    "X.value = X_test\n",
    "res = predict(graph)\n",
    "\n",
    "curr_num = np.sum(y_test == np.argmax(res, axis=1))\n",
    "print \"test acc for all test data : {:.3f} \".format(curr_num/float(test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
